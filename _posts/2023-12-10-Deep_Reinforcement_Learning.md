# Deep Reinforcement Learning

人类擅长解决各种各样的挑战性问题，从低层次的运动控制到高层次的认知任务。我们在DeepMind的目标是创建能够实现类似水平和通用性的人工代理。像人类一样，我们的代理通过自己的学习来实现成功的策略，从而获得最大的长期奖励。这种通过试错来学习的范式，只依赖于奖励或惩罚，被称为强化学习（RL）。同样像人类一样，我们的代理直接从原始输入（如视觉）中构建和学习自己的知识，而不需要任何手工设计的特征或领域启发式方法。这是通过神经网络的深度学习实现的。在DeepMind，我们率先将这些方法结合起来 - 深度强化学习 - 创建了第一个在许多具有挑战性的领域中实现人类水平表现的人工代理。

我们的代理必须不断做出价值判断，以便选择好的行为而不是坏的行为。这种知识由一个Q网络表示，该网络估计代理在采取特定行动后可以预期获得的奖励总额。两年前，我们引入了第一个广泛成功的深度强化学习算法。关键思想是使用深度神经网络来表示Q网络，并训练这个Q网络来预测总奖励。以前尝试将RL与神经网络结合的尝试在很大程度上失败了，因为学习不稳定。为了解决这些不稳定性问题，我们的深度Q网络（DQN）算法存储了代理的所有经验，然后随机抽样和重播这些经验，以提供多样化和去相关的训练数据。我们将DQN应用于学习在Atari 2600游戏机上玩游戏。在每个时间步，代理观察屏幕上的原始像素，对应于游戏得分的奖励信号，并选择一个操纵杆方向。在我们的《自然》论文中，我们在没有任何先前的游戏规则知识的情况下，为50个不同的Atari游戏分别训练了DQN代理。

令人惊讶的是，DQN在应用于其中的50个游戏中几乎达到了人类水平的表现，远远超过了以前的所有方法。 DQN源代码和Atari 2600模拟器可供任何希望自行进行实验的人免费使用。

我们随后在许多方面改进了DQN算法：进一步稳定学习动态；优先考虑重播的经验；规范化、聚合和重新缩放输出。将这些改进措施组合在一起，导致Atari游戏的平均得分提高了300％；现在几乎所有的Atari游戏都实现了人类水平的表现。我们甚至可以训练一个单一的神经网络来学习多个Atari游戏。我们还构建了一个大规模的分布式深度RL系统，称为Gorila，该系统利用Google云平台将训练时间加快了一个数量级；该系统已应用于Google内的推荐系统。

然而，深度Q网络只是解决深度RL问题的一种方法。我们最近介绍了一种更加实用和有效基于异步RL的方法。这种方法利用了标准CPU的多线程能力。这个想法是并行执行多个代理实例，但使用共享模型。这提供了一种可行的替代经验回放的方法，因为并行化也可以使数据多样化和去相关。我们的异步演员评论算法A3C结合了用于选择动作的深度Q网络和深度策略网络。它实现了最先进的结果，使用DQN训练时间的一小部分和Gorila资源消耗的一小部分。通过建立新颖的内在动机和时间抽象规划方法，我们还在最具挑战性的Atari游戏（如蒙特祖玛的复仇）中取得了突破性的成果。

虽然Atari游戏展示了广泛的多样性，但它们仅限于基于2D精灵的视频游戏。我们最近引入了Labyrinth：一个具有挑战性的3D导航和解决问题的环境套件。同样，代理只观察其直接视野范围内的基于像素的输入，并且必须弄清楚地图以发现和利用奖励。

令人惊讶的是，A3C算法在许多Labyrinth任务上实现了人类水平的表现，开箱即用。一种基于情景记忆的替代方法也已被证明是成功的。在未来几个月内，Labyrinth也将作为开源发布。

我们还开发了一些深度RL方法，用于连续控制问题，如机器人操纵和运动。我们的确定性策略梯度算法（DPG）提供了一种连续的DQN类似物，利用Q网络的可微性来解决各种连续控制任务。异步RL在这些领域中也表现良好，并且当与分层控制策略结合使用时，可以解决诸如蚂蚁足球和54维人体障碍滑雪等具有挑战性的问题，而无需任何先前的动力学知识。

围棋是最具挑战性的古典游戏之一。尽管几十年来人们付出了很多努力，但以前的算法只能达到业余水平的表现。我们开发了一种深度RL算法，通过自我对弈的游戏学习价值网络（预测赢家）和策略网络（选择行动）。我们的程序AlphaGo将这些深度神经网络与最先进的树搜索相结合。2015年10月，AlphaGo成为第一个击败职业人类玩家的程序。2016年3月，AlphaGo以4比1的比分击败李世石（过去十年中最强的选手，拥有令人难以置信的18个世界冠军），这场比赛估计有2亿观众观看。

另外，我们还开发了深度RL的游戏理论方法，最终实现了一种超越人类的一对一限制德州扑克玩家。

从Atari到Labyrinth，从操纵到移动，再到扑克甚至围棋，我们的深度强化学习代理在各种具有挑战性的任务上取得了显著进展。我们的目标是继续提高代理的能力，并在医疗保健等重要应用中将其用于对社会产生积极影响。
